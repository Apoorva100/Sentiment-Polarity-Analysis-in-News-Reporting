{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTmr4oQ5Rrcs",
        "outputId": "72a7491c-d2a1-4a60-e822-9b6dbccc386b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package sentence_polarity to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package sentence_polarity is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy of Unigram: 0.6160308526162185\n",
            "Average Precision of Unigram: 0.5376824034334764\n",
            "Average Recall of Unigram: 0.40304847277439687\n",
            "Average F1 Score of Unigram: 0.4534600715617726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy of Negation: 0.7354302398914935\n",
            "Average Precision of Negation: 0.5453947368421053\n",
            "Average Recall of Negation: 0.48671135251472186\n",
            "Average F1 Score of Negation: 0.511063721852422\n",
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Load the sentence polarity data\n",
        "nltk.download('sentence_polarity')\n",
        "from nltk.corpus import sentence_polarity\n",
        "\n",
        "# Load the sentence polarity dataset\n",
        "sentences = list(sentence_polarity.sents())\n",
        "random.shuffle(sentences)\n",
        "\n",
        "# Create a list of documents with the sentence and label\n",
        "documents = [(sent, 'pos' if cat=='pos' else 'neg') for cat in sentence_polarity.categories()\n",
        "            for sent in sentence_polarity.sents(categories=cat)]\n",
        "\n",
        "# Get the most common words as features\n",
        "all_words = [word.lower() for (sent,cat) in documents for word in sent]\n",
        "all_word_freq = nltk.FreqDist(all_words)\n",
        "word_features = list(all_word_freq.keys())[:2000]\n",
        "\n",
        "\n",
        "# this list of negation words includes some \"approximate negators\" like hardly and rarely\n",
        "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
        "\n",
        "\n",
        "# Define document features using bag of words\n",
        "def document_features(document, word_features):\n",
        "    document_words = set([word.lower() for word in document])\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['contains({})'.format(word)] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def NOT_features(document, word_features, negationwords):\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['V_{}'.format(word)] = False\n",
        "        features['V_NOT{}'.format(word)] = False\n",
        "    # go through document words in order\n",
        "    for i in range(0, len(document)):\n",
        "        word = document[i]\n",
        "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
        "            i += 1\n",
        "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
        "        else:\n",
        "            features['V_{}'.format(word)] = (word in word_features)\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "# Extract features for each document\n",
        "featuresets = [(document_features(d, word_features), c) for (d,c) in documents]\n",
        "labels = [c for (d,c) in featuresets]\n",
        "features = [d for (d,c) in featuresets]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Extract features for each document\n",
        "# define the feature sets\n",
        "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]\n",
        "labels_stopWord = [c for (d,c) in NOT_featuresets]\n",
        "features_stopWord = [d for (d,c) in NOT_featuresets]\n",
        "\n",
        "\n",
        "# K-fold cross validation\n",
        "kfold = KFold(n_splits=5)\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "accuracies = []\n",
        "for train_idx, test_idx in kfold.split(features, labels):\n",
        "\n",
        "    # Split data\n",
        "    train_features, test_features = [features[i] for i in train_idx], [features[i] for i in test_idx]\n",
        "    train_labels, test_labels = [labels[i] for i in train_idx], [labels[i] for i in test_idx]\n",
        "\n",
        "    # Train classifier\n",
        "    train_set = list(zip(train_features, train_labels))\n",
        "    test_set = list(zip(test_features, test_labels))\n",
        "    model = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = model.classify_many(test_features)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = nltk.classify.accuracy(model, test_set)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Map string labels to numbers\n",
        "    label_map = {'pos': 1, 'neg': 0}\n",
        "\n",
        "    y_true = [label_map[y] for y in test_labels]\n",
        "    y_pred = [label_map[y] for y in predictions]\n",
        "\n",
        "    # Evaluate scores\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1s.append(f1)\n",
        "\n",
        "# Print average scores\n",
        "print(\"Average Accuracy of Unigram:\", sum(accuracies)/len(accuracies))\n",
        "print(\"Average Precision of Unigram:\", sum(precisions)/len(precisions))\n",
        "print(\"Average Recall of Unigram:\", sum(recalls)/len(recalls))\n",
        "print(\"Average F1 Score of Unigram:\", sum(f1s)/len(f1s))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# K-fold cross validation\n",
        "kfold = KFold(n_splits=5)\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "accuracies = []\n",
        "for train_idx, test_idx in kfold.split(features_stopWord, labels_stopWord):\n",
        "\n",
        "    # Split data\n",
        "    train_features, test_features = [features_stopWord[i] for i in train_idx], [features_stopWord[i] for i in test_idx]\n",
        "    train_labels, test_labels = [labels_stopWord[i] for i in train_idx], [labels_stopWord[i] for i in test_idx]\n",
        "\n",
        "    # Train classifier\n",
        "    train_set = list(zip(train_features, train_labels))\n",
        "    test_set = list(zip(test_features, test_labels))\n",
        "    model = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = model.classify_many(test_features)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = nltk.classify.accuracy(model, test_set)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Map string labels to numbers\n",
        "    label_map = {'pos': 1, 'neg': 0}\n",
        "\n",
        "    y_true = [label_map[y] for y in test_labels]\n",
        "    y_pred = [label_map[y] for y in predictions]\n",
        "\n",
        "    # Evaluate scores\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1s.append(f1)\n",
        "\n",
        "# Print average scores\n",
        "print(\"Average Accuracy of Negation:\", sum(accuracies)/len(accuracies))\n",
        "print(\"Average Precision of Negation:\", sum(precisions)/len(precisions))\n",
        "print(\"Average Recall of Negation:\", sum(recalls)/len(recalls))\n",
        "print(\"Average F1 Score of Negation:\", sum(f1s)/len(f1s))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load and preprocess Fake and True data\n",
        "from google.colab import drive\n",
        "drive.mount(\"/drive\", force_remount=True)\n",
        "import csv\n",
        "import pandas as pd\n",
        "# Read true data\n",
        "real_news = pd.read_csv('/drive/My Drive/Colab Notebooks/IST 664/2023/True.csv')\n",
        "# Read fake data\n",
        "fake_news = pd.read_csv('/drive/My Drive/Colab Notebooks/IST 664/2023/Fake.csv')\n",
        "\n",
        "\n",
        "\n",
        "# Analyze first 50 fake news articles for negation\n",
        "fake_analysis_negation = []\n",
        "for index, row in fake_news[:50].iterrows():\n",
        "    pos_count = 0\n",
        "    neg_count = 0\n",
        "    for sentence in row['text'].split('.'):\n",
        "        if model.classify(NOT_features(sentence.split(), word_features, negationwords)) == 'pos':\n",
        "            pos_count += 1\n",
        "        else:\n",
        "            neg_count += 1\n",
        "    fake_analysis_negation.append([row['text'], pos_count, neg_count])\n",
        "\n",
        "\n",
        "\n",
        "# Write Negation unigram fake news analysis to CSV\n",
        "header = ['text', 'the number of positive sentences in text', 'the number of negative sentences in text']\n",
        "with open('fake_analysis_negation.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(fake_analysis_negation)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Analyze first 50 real news articles for negation\n",
        "real_analysis_negation = []\n",
        "for index, row in real_news[:50].iterrows():\n",
        "    pos_count = 0\n",
        "    neg_count = 0\n",
        "    for sentence in row['text'].split('.'):\n",
        "        if model.classify(NOT_features(sentence.split(), word_features , negationwords)) == 'pos':\n",
        "            pos_count += 1\n",
        "        else:\n",
        "            neg_count += 1\n",
        "    real_analysis_negation.append([row['text'], pos_count, neg_count])\n",
        "\n",
        "\n",
        "\n",
        "# Write negation real news analysis to CSV\n",
        "with open('real_analysis_negation.csv', 'w') as f:\n",
        "   writer = csv.writer(f)\n",
        "   writer.writerow(header)\n",
        "   writer.writerows(real_analysis_negation)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on comparing the evaluation metrics between the unigram and negation models, we can make a few observations:\n",
        "The negation model has higher accuracy than the unigram model (0.735 vs 0.616). Adding explicit negation features improved accuracy.\n",
        "Precision is slightly higher for unigrams, while recall is better for negation. This indicates negation reduced some false positives but at the cost of fewer true positives.\n",
        "The F1 score, which balances precision and recall, is higher for the negation model compared to unigrams (0.511 vs 0.453). The overall F1 improvement shows the benefits of modeling negation outweigh the precision drop.\n",
        "\n",
        "In summary, the negation model leads to large gains in accuracy and modest gains in F1 score. This suggests explicitly handling negation sentiment reversals is beneficial for this dataset, despite a small reduction in precision. The accuracy and F1 improvements show the advantages of adding negation features compared to just unigram bag-of-words."
      ],
      "metadata": {
        "id": "vaE4ULDcmfPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------"
      ],
      "metadata": {
        "id": "IytCIB3_qYQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to use the sentence polarity corpus from NLTK for this sentiment analysis assignment for a few key reasons:\n",
        "\n",
        "It provides a nicely preprocessed sentiment dataset for training and evaluation. The sentences are labeled with positive/negative sentiment tags.\n",
        "\n",
        "The sentences come from movie reviews so they represent realistic language use and opinions versus synthetic data.\n",
        "\n",
        "It has a good size for experimentation - over 10,000 sentences split between train and test sets. Large enough for meaningful results.\n",
        "\n",
        "The domain of movie reviews is relevant for common sentiment analysis applications around entertainment, product reviews, etc.\n",
        "\n",
        "Some key advantages this dataset provides:\n",
        "\n",
        "Realistic language examples for training sentiment classifiers.\n",
        "\n",
        "Cleanly labeled data for supervised learning.\n",
        "\n",
        "Domain-specific text from movie reviews.\n",
        "\n",
        "Relevance to common sentiment analysis applications.\n",
        "\n",
        "The sentence polarity corpus has been a great way to quickly get up and running with core sentiment analysis techniques. The dataset quality and characteristics make it very suitable for this type of analysis."
      ],
      "metadata": {
        "id": "Y9fMAQ1EqGkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load CSV results\n",
        "fake_negation = pd.read_csv('/content/fake_analysis_negation.csv')\n",
        "real_negation = pd.read_csv('/content/real_analysis_negation.csv')\n",
        "\n",
        "\n",
        "# Compare positive and negative sentences\n",
        "print(\"Average Pos Sentences:\")\n",
        "print(\"Fake:\", fake_negation['the number of positive sentences in text'].mean())\n",
        "print(\"Real:\", real_negation['the number of positive sentences in text'].mean())\n",
        "\n",
        "print(\"\\nAverage Neg Sentences:\")\n",
        "print(\"Fake:\", fake_negation['the number of negative sentences in text'].mean())\n",
        "print(\"Real:\", real_negation['the number of negative sentences in text'].mean())\n",
        "\n",
        "# Explanation and discussion\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSji2ZBhkjRV",
        "outputId": "43a5ceff-a034-4f86-a6ce-9bd2d0e4660f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Pos Sentences:\n",
            "Fake: 8.42\n",
            "Real: 9.3\n",
            "\n",
            "Average Neg Sentences:\n",
            "Fake: 13.48\n",
            "Real: 16.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the output showing the average positive and negative sentences in the fake vs real news CSVs, we can make a few observations:\n",
        "The real news contains more positive sentences on average than the fake news (9.3 vs 8.42)\n",
        "The real news contains significantly more negative sentences on average compared to the fake news (16.66 vs 13.48).\n",
        "This suggests:\n",
        "The fake news articles use less positive sentiment language overall compared to real news. This could indicate fake news tends to be more neutral in tone.\n",
        "The fake news articles contain much less negative sentiment language on average. The lower amount of negative sentences could suggest fake news avoids strong negative language.\n",
        "The combination of less positive and fewer negative sentences makes the fake news sentiment more neutral compared to real news.\n",
        "So in summary, the fake news sentiment appears more neutral and avoided extremes of positive or negative sentiment based on this analysis. This could help fake news avoid seeming too over-the-top or emotional compared to real reporting."
      ],
      "metadata": {
        "id": "cdIeYwatlL2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 20 rows of the csv files\n",
        "\n",
        "df1 = pd.read_csv('/content/fake_analysis_negation.csv')\n",
        "print(df1.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3Bv-PMvn4Oj",
        "outputId": "7fa6f22d-1eca-4f4c-cf20-6109d02887e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  \\\n",
            "0   Donald Trump just couldn t wish all Americans ...   \n",
            "1   House Intelligence Committee Chairman Devin Nu...   \n",
            "2   On Friday, it was revealed that former Milwauk...   \n",
            "3   On Christmas day, Donald Trump announced that ...   \n",
            "4   Pope Francis used his annual Christmas Day mes...   \n",
            "5   The number of cases of cops brutalizing and ki...   \n",
            "6   Donald Trump spent a good portion of his day a...   \n",
            "7   In the wake of yet another court decision that...   \n",
            "8   Many people have raised the alarm regarding th...   \n",
            "9   Just when you might have thought we d get a br...   \n",
            "10  A centerpiece of Donald Trump s campaign, and ...   \n",
            "11  Republicans are working overtime trying to sel...   \n",
            "12  Republicans have had seven years to come up wi...   \n",
            "13  The media has been talking all day about Trump...   \n",
            "14  Abigail Disney is an heiress with brass ovarie...   \n",
            "15  Donald Trump just signed the GOP tax scam into...   \n",
            "16  A new animatronic figure in the Hall of Presid...   \n",
            "17  Trump supporters and the so-called president s...   \n",
            "18  Right now, the whole world is looking at the s...   \n",
            "19  Senate Majority Whip John Cornyn (R-TX) though...   \n",
            "\n",
            "    the number of positive sentences in text  \\\n",
            "0                                         10   \n",
            "1                                          6   \n",
            "2                                         12   \n",
            "3                                          8   \n",
            "4                                         10   \n",
            "5                                         10   \n",
            "6                                         12   \n",
            "7                                          6   \n",
            "8                                         12   \n",
            "9                                         11   \n",
            "10                                         9   \n",
            "11                                         4   \n",
            "12                                        10   \n",
            "13                                         7   \n",
            "14                                        11   \n",
            "15                                         8   \n",
            "16                                         7   \n",
            "17                                         9   \n",
            "18                                         9   \n",
            "19                                        10   \n",
            "\n",
            "    the number of negative sentences in text  \n",
            "0                                         14  \n",
            "1                                         15  \n",
            "2                                         31  \n",
            "3                                         26  \n",
            "4                                         11  \n",
            "5                                          5  \n",
            "6                                          9  \n",
            "7                                          6  \n",
            "8                                          9  \n",
            "9                                          7  \n",
            "10                                         7  \n",
            "11                                         9  \n",
            "12                                        18  \n",
            "13                                        12  \n",
            "14                                        22  \n",
            "15                                         9  \n",
            "16                                        27  \n",
            "17                                        10  \n",
            "18                                        10  \n",
            "19                                        18  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('/content/real_analysis_negation.csv')\n",
        "print(df2.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua05CwVPoC5I",
        "outputId": "d3c30a29-3d8f-463c-eeb6-530ed3f52f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  \\\n",
            "0   WASHINGTON (Reuters) - The head of a conservat...   \n",
            "1   WASHINGTON (Reuters) - Transgender people will...   \n",
            "2   WASHINGTON (Reuters) - The special counsel inv...   \n",
            "3   WASHINGTON (Reuters) - Trump campaign adviser ...   \n",
            "4   SEATTLE/WASHINGTON (Reuters) - President Donal...   \n",
            "5   WEST PALM BEACH, Fla./WASHINGTON (Reuters) - T...   \n",
            "6   WEST PALM BEACH, Fla (Reuters) - President Don...   \n",
            "7   The following statements were posted to the ve...   \n",
            "8   The following statements were posted to the ve...   \n",
            "9   WASHINGTON (Reuters) - Alabama Secretary of St...   \n",
            "10  (Reuters) - Alabama officials on Thursday cert...   \n",
            "11  NEW YORK/WASHINGTON (Reuters) - The new U.S. t...   \n",
            "12  The following statements were posted to the ve...   \n",
            "13  The following statements were posted to the ve...   \n",
            "14   (In Dec. 25 story, in second paragraph, corre...   \n",
            "15  (Reuters) - A lottery drawing to settle a tied...   \n",
            "16  WASHINGTON (Reuters) - A Georgian-American bus...   \n",
            "17  The following statements were posted to the ve...   \n",
            "18  (Reuters) - A U.S. appeals court in Washington...   \n",
            "19  (Reuters) - A gift-wrapped package addressed t...   \n",
            "\n",
            "    the number of positive sentences in text  \\\n",
            "0                                         10   \n",
            "1                                         17   \n",
            "2                                         14   \n",
            "3                                         10   \n",
            "4                                         16   \n",
            "5                                         15   \n",
            "6                                          8   \n",
            "7                                          3   \n",
            "8                                          5   \n",
            "9                                          1   \n",
            "10                                        10   \n",
            "11                                        11   \n",
            "12                                         4   \n",
            "13                                        10   \n",
            "14                                         3   \n",
            "15                                         8   \n",
            "16                                        17   \n",
            "17                                         5   \n",
            "18                                         6   \n",
            "19                                         4   \n",
            "\n",
            "    the number of negative sentences in text  \n",
            "0                                         40  \n",
            "1                                         21  \n",
            "2                                          9  \n",
            "3                                         12  \n",
            "4                                         43  \n",
            "5                                         22  \n",
            "6                                         29  \n",
            "7                                         10  \n",
            "8                                          6  \n",
            "9                                          4  \n",
            "10                                        17  \n",
            "11                                        27  \n",
            "12                                         5  \n",
            "13                                        12  \n",
            "14                                        18  \n",
            "15                                        11  \n",
            "16                                        29  \n",
            "17                                         8  \n",
            "18                                        17  \n",
            "19                                        12  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "8ctH7AjglzXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unigram Model\n",
        "This model uses a bag-of-words representation with unigram features.\n",
        "The features are individual words from the sentences.\n",
        "Each sentence is represented by a feature vector indicating the presence/absence of the top 2000 most frequent words.\n",
        "No preprocessing is done - all words are used as features as-is.\n",
        "\n",
        "Negation Model\n",
        "This model builds on top of the unigram features.\n",
        "Additional negation features are added to capture negation.\n",
        "Negation words like \"not\", \"no\", etc. are defined.\n",
        "When a negation word appears, the next word is marked as negated.\n",
        "So \"not good\" would have the \"good\" feature negated.\n",
        "This aims to handle negation more explicitly.\n",
        "\n",
        "The negation model should help improve performance by better encoding sentiment reversing negation words."
      ],
      "metadata": {
        "id": "CXdRsFgrlzvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "oTqtmtJrmBsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There is no data pre-processing tasks to be conducted in Sentiment Analysis. Words should not be convereted into lowercase to preserve capitalized words. Removal od non-words also should not be done to preserve punctuation.Removal of stop words can be done separately as a feature set itself. However, tokenization should be done."
      ],
      "metadata": {
        "id": "g7zLtP_OcZLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "wTZtq0J4nfJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Here are some key things I learned from working on this sentiment analysis assignment:\n",
        "\n",
        "Preprocessing like normalization and handling negation is very important for sentiment analysis. My initial bag-of-words model would have benefited from more preprocessing.\n",
        "\n",
        "Adding features like negation handling can significantly improve model performance for sentiment classification, despite using a simple Naive Bayes classifier.\n",
        "\n",
        "Validation techniques like k-fold cross-validation are essential for properly evaluating and comparing models.\n",
        "Sentiment analysis provides an interesting way to compare qualities like positive/negative language in different text sources like real vs fake news.\n",
        "\n",
        "There are lots of options for enhancing sentiment classifiers like utilizing lexicons, handling stop words, handling negation scope, etc. Lots of room to experiment.\n",
        "Sentiment analysis has many practical applications in fields like marketing, social media monitoring, and reviewing systems. It can provide business insights.\n",
        "\n",
        "There are some key challenges like understanding context, sarcasm, ambiguous language that require more advanced techniques.\n",
        "\n",
        "Overall, it was great hands-on practice for sentiment analysis techniques. I have a much better understanding now of building, evaluating, and applying these types of models.\n"
      ],
      "metadata": {
        "id": "vzoNQ71-dDw3"
      }
    }
  ]
}